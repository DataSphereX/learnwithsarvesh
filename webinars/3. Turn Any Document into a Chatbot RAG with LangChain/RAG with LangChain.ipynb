{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe66004d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e22e6",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8dea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded from: D:\\Mentoring\\learwithsarvesh\\webinars\\3. 9th Nov - From Python to LangChain Build AI Apps in 12 Weeks\\.env\n",
      "API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = r\"D:\\Mentoring\\learwithsarvesh\\webinars\\3. 9th Nov - From Python to LangChain Build AI Apps in 12 Weeks\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"âœ… Loaded from:\", dotenv_path)\n",
    "print(\"API key loaded:\", os.getenv(\"OPENAI_API_KEY\") is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf898a",
   "metadata": {},
   "source": [
    "### Load and Split PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fce8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 30 pages and split into 113 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF\n",
    "pdf_path = r\"D:\\Mentoring\\learwithsarvesh\\webinars\\3. 9th Nov - From Python to LangChain Build AI Apps in 12 Weeks\\HDFC-Life-Group-Term-Life-Policy.pdf\"\n",
    "\n",
    "# Load document\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Split text into chunks for processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"âœ… Loaded {len(pages)} pages and split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fd8ed",
   "metadata": {},
   "source": [
    "### Create Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0383ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector database created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and store in vector database\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai.api_key )\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "print(\"âœ… Vector database created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c89006",
   "metadata": {},
   "source": [
    "### Build the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615634a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversational chain ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "# Build the conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"âœ… Conversational chain ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb46b4",
   "metadata": {},
   "source": [
    "### Start Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e380b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarve\\AppData\\Local\\Temp\\ipykernel_1724\\4130575769.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– AI: Hello! How can I assist you today?\n",
      "\n",
      "ðŸ¤– AI: The toll-free number for HDFC Life Insurance is 1800 266 9777.\n",
      "\n",
      "ðŸ‘‹ Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nðŸ§‘ You: \")\n",
    "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"ðŸ‘‹ Chat ended.\")\n",
    "        break\n",
    "\n",
    "    result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(f\"ðŸ¤– AI: {result['answer']}\\n\")\n",
    "    chat_history.append((query, result[\"answer\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
